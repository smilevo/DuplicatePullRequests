# -*- coding: utf-8 -*-
"""VR4_MaxPooling_SiameseBert_3329_Full_Data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t2uYau4L8f4D68R67k_Cy-eyt8ZuliDk
"""

!pip install transformers

"""# ***Importing the tools***"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
import re
warnings.filterwarnings('ignore')

"""# ***Importing the dataset from Drive***"""

from google.colab import drive 
drive.mount('/content/gdrive')

df=pd.read_csv('/content/gdrive/MyDrive/VR_4/V4_DATA.csv', delimiter=';', encoding='cp437')

#lire la base des données ligne par ligne 
pd.set_option('display.max_rows',df.shape[0]+1)
#df

#voir les valeurs manquantes
df.isnull().sum().sort_values(ascending=False)

"""# ***Loading the Pre-trained BERT model***"""

import time
start = time.time()

model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

"""# ***Remove stop words***"""

df['Title_1']= df['Title_1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

df['Title_2']= df['Title_2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

df['Content_1']= df['Content_1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

df['Content_2']= df['Content_2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

"""# ***Remove Digits :***"""

df['Title_1']= df['Title_1'].str.replace(r'0', '')
df['Title_1']= df['Title_1'].str.replace(r'1', '')
df['Title_1']= df['Title_1'].str.replace(r'2', '')
df['Title_1']= df['Title_1'].str.replace(r'3', '')
df['Title_1']= df['Title_1'].str.replace(r'4', '')
df['Title_1']= df['Title_1'].str.replace(r'5', '')
df['Title_1']= df['Title_1'].str.replace(r'6', '')
df['Title_1']= df['Title_1'].str.replace(r'7', '')
df['Title_1']= df['Title_1'].str.replace(r'8', '')
df['Title_1']= df['Title_1'].str.replace(r'9', '')

df['Title_2']= df['Title_2'].str.replace(r'0', '')
df['Title_2']= df['Title_2'].str.replace(r'1', '')
df['Title_2']= df['Title_2'].str.replace(r'2', '')
df['Title_2']= df['Title_2'].str.replace(r'3', '')
df['Title_2']= df['Title_2'].str.replace(r'4', '')
df['Title_2']= df['Title_2'].str.replace(r'5', '')
df['Title_2']= df['Title_2'].str.replace(r'6', '')
df['Title_2']= df['Title_2'].str.replace(r'7', '')
df['Title_2']= df['Title_2'].str.replace(r'8', '')
df['Title_2']= df['Title_2'].str.replace(r'9', '')

df['Content_1']= df['Content_1'].str.replace(r'0', '')
df['Content_1']= df['Content_1'].str.replace(r'1', '')
df['Content_1']= df['Content_1'].str.replace(r'2', '')
df['Content_1']= df['Content_1'].str.replace(r'3', '')
df['Content_1']= df['Content_1'].str.replace(r'4', '')
df['Content_1']= df['Content_1'].str.replace(r'5', '')
df['Content_1']= df['Content_1'].str.replace(r'6', '')
df['Content_1']= df['Content_1'].str.replace(r'7', '')
df['Content_1']= df['Content_1'].str.replace(r'8', '')
df['Content_1']= df['Content_1'].str.replace(r'9', '')

df['Content_2']= df['Content_2'].str.replace(r'0', '')
df['Content_2']= df['Content_2'].str.replace(r'1', '')
df['Content_2']= df['Content_2'].str.replace(r'2', '')
df['Content_2']= df['Content_2'].str.replace(r'3', '')
df['Content_2']= df['Content_2'].str.replace(r'4', '')
df['Content_2']= df['Content_2'].str.replace(r'5', '')
df['Content_2']= df['Content_2'].str.replace(r'6', '')
df['Content_2']= df['Content_2'].str.replace(r'7', '')
df['Content_2']= df['Content_2'].str.replace(r'8', '')
df['Content_2']= df['Content_2'].str.replace(r'9', '')

"""# ***Remove special characters***"""

df['Title_1']= df['Title_1'].str.replace(r'/', '')
df['Title_1']= df['Title_1'].str.replace(r'@', '')
df['Title_1']= df['Title_1'].str.replace(r'!', '')
df['Title_1']= df['Title_1'].str.replace(r'+', '')
df['Title_1']= df['Title_1'].str.replace(r'-', '')
df['Title_1']= df['Title_1'].str.replace(r'/', '')
df['Title_1']= df['Title_1'].str.replace(r':', '')
df['Title_1']= df['Title_1'].str.replace(r';', '')
df['Title_1']= df['Title_1'].str.replace(r'>', '')
df['Title_1']= df['Title_1'].str.replace(r'=', '')
df['Title_1']= df['Title_1'].str.replace(r'<', '')
df['Title_1']= df['Title_1'].str.replace(r'(', '')
df['Title_1']= df['Title_1'].str.replace(r')', '')
df['Title_1']= df['Title_1'].str.replace(r'#', '')
df['Title_1']= df['Title_1'].str.replace(r'$', '')
df['Title_1']= df['Title_1'].str.replace(r'*', '')
df['Title_1']= df['Title_1'].str.replace(r'_', '')
df['Title_1']= df['Title_1'].str.replace(r']', '')
df['Title_1']= df['Title_1'].str.replace(r'[', '')
df['Title_1']= df['Title_1'].str.replace(r'{', '')
df['Title_1']= df['Title_1'].str.replace(r'}', '')
df['Title_1']= df['Title_1'].str.replace(r'"', '')
df['Title_1']= df['Title_1'].str.replace(r'&', '')
df['Title_1']= df['Title_1'].str.replace(r'~', '')
df['Title_1']= df['Title_1'].str.replace(r'^', '')
df['Title_1']= df['Title_1'].str.replace(r'°', '')
df['Title_1']= df['Title_1'].str.replace(r'?', '')
df['Title_1']= df['Title_1'].str.replace(r'%', '')

df['Title_2']= df['Title_2'].str.replace(r'/', '')
df['Title_2']= df['Title_2'].str.replace(r'@', '')
df['Title_2']= df['Title_2'].str.replace(r'!', '')
df['Title_2']= df['Title_2'].str.replace(r'+', '')
df['Title_2']= df['Title_2'].str.replace(r'-', '')
df['Title_2']= df['Title_2'].str.replace(r'/', '')
df['Title_2']= df['Title_2'].str.replace(r':', '')
df['Title_2']= df['Title_2'].str.replace(r';', '')
df['Title_2']= df['Title_2'].str.replace(r'>', '')
df['Title_2']= df['Title_2'].str.replace(r'=', '')
df['Title_2']= df['Title_2'].str.replace(r'<', '')
df['Title_2']= df['Title_2'].str.replace(r'(', '')
df['Title_2']= df['Title_2'].str.replace(r')', '')
df['Title_2']= df['Title_2'].str.replace(r'#', '')
df['Title_2']= df['Title_2'].str.replace(r'$', '')
df['Title_2']= df['Title_2'].str.replace(r'*', '')
df['Title_2']= df['Title_2'].str.replace(r'_', '')
df['Title_2']= df['Title_2'].str.replace(r']', '')
df['Title_2']= df['Title_2'].str.replace(r'[', '')
df['Title_2']= df['Title_2'].str.replace(r'{', '')
df['Title_2']= df['Title_2'].str.replace(r'}', '')
df['Title_2']= df['Title_2'].str.replace(r'"', '')
df['Title_2']= df['Title_2'].str.replace(r'&', '')
df['Title_2']= df['Title_2'].str.replace(r'~', '')
df['Title_2']= df['Title_2'].str.replace(r'^', '')
df['Title_2']= df['Title_2'].str.replace(r'°', '')
df['Title_2']= df['Title_2'].str.replace(r'?', '')
df['Title_2']= df['Title_2'].str.replace(r'%', '')

df['Content_1']= df['Content_1'].str.replace(r'/', '')
df['Content_1']= df['Content_1'].str.replace(r'@', '')
df['Content_1']= df['Content_1'].str.replace(r'!', '')
df['Content_1']= df['Content_1'].str.replace(r'+', '')
df['Content_1']= df['Content_1'].str.replace(r'-', '')
df['Content_1']= df['Content_1'].str.replace(r'/', '')
df['Content_1']= df['Content_1'].str.replace(r':', '')
df['Content_1']= df['Content_1'].str.replace(r';', '')
df['Content_1']= df['Content_1'].str.replace(r'>', '')
df['Content_1']= df['Content_1'].str.replace(r'=', '')
df['Content_1']= df['Content_1'].str.replace(r'<', '')
df['Content_1']= df['Content_1'].str.replace(r'(', '')
df['Content_1']= df['Content_1'].str.replace(r')', '')
df['Content_1']= df['Content_1'].str.replace(r'#', '')
df['Content_1']= df['Content_1'].str.replace(r'$', '')
df['Content_1']= df['Content_1'].str.replace(r'*', '')
df['Content_1']= df['Content_1'].str.replace(r'_', '')
df['Content_1']= df['Content_1'].str.replace(r']', '')
df['Content_1']= df['Content_1'].str.replace(r'[', '')
df['Content_1']= df['Content_1'].str.replace(r'{', '')
df['Content_1']= df['Content_1'].str.replace(r'}', '')
df['Content_1']= df['Content_1'].str.replace(r'"', '')
df['Content_1']= df['Content_1'].str.replace(r'&', '')
df['Content_1']= df['Content_1'].str.replace(r'~', '')
df['Content_1']= df['Content_1'].str.replace(r'^', '')
df['Content_1']= df['Content_1'].str.replace(r'°', '')
df['Content_1']= df['Content_1'].str.replace(r'?', '')
df['Content_1']= df['Content_1'].str.replace(r'%', '')

df['Content_2']= df['Content_2'].str.replace(r'/', '')
df['Content_2']= df['Content_2'].str.replace(r'@', '')
df['Content_2']= df['Content_2'].str.replace(r'!', '')
df['Content_2']= df['Content_2'].str.replace(r'+', '')
df['Content_2']= df['Content_2'].str.replace(r'-', '')
df['Content_2']= df['Content_2'].str.replace(r'/', '')
df['Content_2']= df['Content_2'].str.replace(r':', '')
df['Content_2']= df['Content_2'].str.replace(r';', '')
df['Content_2']= df['Content_2'].str.replace(r'>', '')
df['Content_2']= df['Content_2'].str.replace(r'=', '')
df['Content_2']= df['Content_2'].str.replace(r'<', '')
df['Content_2']= df['Content_2'].str.replace(r'(', '')
df['Content_2']= df['Content_2'].str.replace(r')', '')
df['Content_2']= df['Content_2'].str.replace(r'#', '')
df['Content_2']= df['Content_2'].str.replace(r'$', '')
df['Content_2']= df['Content_2'].str.replace(r'*', '')
df['Content_2']= df['Content_2'].str.replace(r'_', '')
df['Content_2']= df['Content_2'].str.replace(r']', '')
df['Content_2']= df['Content_2'].str.replace(r'[', '')
df['Content_2']= df['Content_2'].str.replace(r'{', '')
df['Content_2']= df['Content_2'].str.replace(r'}', '')
df['Content_2']= df['Content_2'].str.replace(r'"', '')
df['Content_2']= df['Content_2'].str.replace(r'&', '')
df['Content_2']= df['Content_2'].str.replace(r'~', '')
df['Content_2']= df['Content_2'].str.replace(r'^', '')
df['Content_2']= df['Content_2'].str.replace(r'°', '')
df['Content_2']= df['Content_2'].str.replace(r'?', '')
df['Content_2']= df['Content_2'].str.replace(r'%', '')

#lire la base des données ligne par ligne 
pd.set_option('display.max_rows',df.shape[0]+1)
#df

"""# ***Function: _get_segments***"""

def _get_segments3(tokens, max_seq_length):
    """Segments: 0 for the first sequence, 1 for the second"""
    if len(tokens)>max_seq_length:
        raise IndexError("Token length more than max seq length!")
    segments = []
    first_sep = False
    current_segment_id = 0 
    for token in tokens:
        segments.append(current_segment_id)
        #print(token)
        if token == 102:
            #if first_sep:
                #first_sep = False 
            #else:
           current_segment_id = 1
    return segments + [0] * (max_seq_length - len(tokens))

"""# ***Batchs***"""

df1=df[:200]
df2=df[200:400]
df3=df[400:600]
df4=df[600:800]
df5=df[800:1000]
df6=df[1000:1200]
df7=df[1200:1400]
df8=df[1400:1600]
df9=df[1600:1800]
df10=df[1800:2000]
df11=df[2000:2200]
df12=df[2200:2400]
df13=df[2400:2600]
df14=df[2600:2800]
df15=df[2800:3000]
df16=df[3000:3200]
df17=df[3200:]

"""# ***Pull1***

# df1
"""

#Tokenization
pair1 = df1['Title_1'] + df1['Content_1'] + [" [SEP] "] 
tokenized1 = pair1.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len1 = 0                 # padding all lists to the same size
for i in tokenized1.values:
    if len(i) > max_len1:
        max_len1 = len(i)
max_len1 =300
padded1 = np.array([i + [0]*(max_len1-len(i)) for i in tokenized1.values])

np.array(padded1).shape # Dimensions of the padded variable
#Masking
attention_mask1 = np.where(padded1 != 0, 1, 0)
attention_mask1.shape
input_ids1 = torch.tensor(padded1)  
attention_mask1 = torch.tensor(attention_mask1)

#Running the model () function through BERT
input_segments1 = np.array([_get_segments3(token, max_len1)for token in tokenized1.values])
token_type_ids1 = torch.tensor(input_segments1)
input_segments1 = torch.tensor(input_segments1)

with torch.no_grad():
    last_hidden_states1 = model(input_ids1, attention_mask=attention_mask1, token_type_ids=input_segments1)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features1 = last_hidden_states1[0][:,0,:].numpy()
#features1

"""# *df2*"""

#Tokenization
pair2= df2['Title_1'] + df2['Content_1'] + [" [SEP] "] 
tokenized2 = pair2.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len2 = 0                 # padding all lists to the same size
for i in tokenized2.values:
    if len(i) > max_len2:
        max_len2 = len(i)
max_len2 =300
padded2 = np.array([i + [0]*(max_len2-len(i)) for i in tokenized2.values])

np.array(padded2).shape # Dimensions of the padded variable
#Masking
attention_mask2 = np.where(padded2 != 0, 1, 0)
attention_mask2.shape
input_ids2 = torch.tensor(padded2)  
attention_mask2 = torch.tensor(attention_mask2)

#Running the model () function through BERT
input_segments2= np.array([_get_segments3(token, max_len2)for token in tokenized2.values])
token_type_ids2 = torch.tensor(input_segments2)
input_segments2 = torch.tensor(input_segments2)

with torch.no_grad():
    last_hidden_states2 = model(input_ids2, attention_mask=attention_mask2, token_type_ids=input_segments2)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features2 = last_hidden_states2[0][:,0,:].numpy()
#features2

"""# *df3*"""

#Tokenization
pair3= df3['Title_1'] + df3['Content_1'] + [" [SEP] "] 
tokenized3 = pair3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len3 = 0                 # padding all lists to the same size
for i in tokenized3.values:
    if len(i) > max_len3:
        max_len3 = len(i)
max_len3 =300
padded3 = np.array([i + [0]*(max_len3-len(i)) for i in tokenized3.values])

np.array(padded3).shape # Dimensions of the padded variable
#Masking
attention_mask3 = np.where(padded3 != 0, 1, 0)
attention_mask3.shape
input_ids3 = torch.tensor(padded3)  
attention_mask3 = torch.tensor(attention_mask3)

#Running the model () function through BERT
input_segments3= np.array([_get_segments3(token, max_len3)for token in tokenized3.values])
token_type_ids3 = torch.tensor(input_segments3)
input_segments3 = torch.tensor(input_segments3)

with torch.no_grad():
    last_hidden_states3 = model(input_ids3, attention_mask=attention_mask3, token_type_ids=input_segments3)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features3 = last_hidden_states3[0][:,0,:].numpy()
#features3

"""# *df4*"""

#Tokenization
pair4= df4['Title_1'] + df4['Content_1'] + [" [SEP] "] 
tokenized4 = pair4.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len4 = 0                 # padding all lists to the same size
for i in tokenized4.values:
    if len(i) > max_len4:
        max_len4 = len(i)
max_len4 =300
padded4 = np.array([i + [0]*(max_len4-len(i)) for i in tokenized4.values])

np.array(padded4).shape # Dimensions of the padded variable
#Masking
attention_mask4 = np.where(padded4 != 0, 1, 0)
attention_mask4.shape
input_ids4 = torch.tensor(padded4)  
attention_mask4 = torch.tensor(attention_mask4)

#Running the model () function through BERT
input_segments4= np.array([_get_segments3(token, max_len4)for token in tokenized4.values])
token_type_ids4 = torch.tensor(input_segments4)
input_segments4 = torch.tensor(input_segments4)

with torch.no_grad():
    last_hidden_states4 = model(input_ids4, attention_mask=attention_mask4, token_type_ids=input_segments4)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features4 = last_hidden_states4[0][:,0,:].numpy()
#features4

"""# *df5*"""

#Tokenization
pair5= df5['Title_1'] + df5['Content_1'] + [" [SEP] "] 
tokenized5 = pair5.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len5 = 0                 # padding all lists to the same size
for i in tokenized5.values:
    if len(i) > max_len5:
        max_len5 = len(i)
max_len5 =300
padded5 = np.array([i + [0]*(max_len5-len(i)) for i in tokenized5.values])

np.array(padded5).shape # Dimensions of the padded variable
#Masking
attention_mask5 = np.where(padded5 != 0, 1, 0)
attention_mask5.shape
input_ids5 = torch.tensor(padded5)  
attention_mask5 = torch.tensor(attention_mask5)

#Running the model () function through BERT
input_segments5= np.array([_get_segments3(token, max_len5)for token in tokenized5.values])
token_type_ids5 = torch.tensor(input_segments5)
input_segments5 = torch.tensor(input_segments5)

with torch.no_grad():
    last_hidden_states5 = model(input_ids5, attention_mask=attention_mask5, token_type_ids=input_segments5)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features5 = last_hidden_states5[0][:,0,:].numpy()
#features5

"""# *df6*"""

#Tokenization
pair6= df6['Title_1'] + df6['Content_1'] + [" [SEP] "] 
tokenized6 = pair6.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len6 = 0                 # padding all lists to the same size
for i in tokenized6.values:
    if len(i) > max_len6:
        max_len6 = len(i)
max_len6 =300
padded6 = np.array([i + [0]*(max_len6-len(i)) for i in tokenized6.values])

np.array(padded6).shape # Dimensions of the padded variable
#Masking
attention_mask6 = np.where(padded6 != 0, 1, 0)
attention_mask6.shape
input_ids6 = torch.tensor(padded6)  
attention_mask6 = torch.tensor(attention_mask6)

#Running the model () function through BERT
input_segments6= np.array([_get_segments3(token, max_len6)for token in tokenized6.values])
token_type_ids6 = torch.tensor(input_segments6)
input_segments6 = torch.tensor(input_segments6)

with torch.no_grad():
    last_hidden_states6 = model(input_ids6, attention_mask=attention_mask6, token_type_ids=input_segments6)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features6 = last_hidden_states6[0][:,0,:].numpy()
#features6

"""# *df7*"""

#Tokenization
pair7= df7['Title_1'] + df7['Content_1'] + [" [SEP] "] 
tokenized7 = pair7.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len7 = 0                 # padding all lists to the same size
for i in tokenized7.values:
    if len(i) > max_len7:
        max_len7 = len(i)
max_len7 =300
padded7 = np.array([i + [0]*(max_len7-len(i)) for i in tokenized7.values])

np.array(padded7).shape # Dimensions of the padded variable
#Masking
attention_mask7 = np.where(padded7 != 0, 1, 0)
attention_mask7.shape
input_ids7 = torch.tensor(padded7)  
attention_mask7 = torch.tensor(attention_mask7)

#Running the model () function through BERT
input_segments7= np.array([_get_segments3(token, max_len7)for token in tokenized7.values])
token_type_ids7 = torch.tensor(input_segments7)
input_segments7 = torch.tensor(input_segments7)

with torch.no_grad():
    last_hidden_states7 = model(input_ids7, attention_mask=attention_mask7, token_type_ids=input_segments7)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features7 = last_hidden_states7[0][:,0,:].numpy()
#features7

"""# *df8*"""

#Tokenization
pair8= df8['Title_1'] + df8['Content_1'] + [" [SEP] "] 
tokenized8 = pair8.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len8 = 0                 # padding all lists to the same size
for i in tokenized8.values:
    if len(i) > max_len8:
        max_len8 = len(i)
max_len8 =300
padded8 = np.array([i + [0]*(max_len8-len(i)) for i in tokenized8.values])

np.array(padded8).shape # Dimensions of the padded variable
#Masking
attention_mask8 = np.where(padded8 != 0, 1, 0)
attention_mask8.shape
input_ids8 = torch.tensor(padded8)  
attention_mask8 = torch.tensor(attention_mask8)

#Running the model () function through BERT
input_segments8= np.array([_get_segments3(token, max_len8)for token in tokenized8.values])
token_type_ids8 = torch.tensor(input_segments8)
input_segments8 = torch.tensor(input_segments8)

with torch.no_grad():
    last_hidden_states8 = model(input_ids8, attention_mask=attention_mask8, token_type_ids=input_segments8)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features8 = last_hidden_states8[0][:,0,:].numpy()
#features8

"""# *df9*"""

#Tokenization
pair9= df9['Title_1'] + df9['Content_1'] + [" [SEP] "] 
tokenized9 = pair9.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len9 = 0                 # padding all lists to the same size
for i in tokenized9.values:
    if len(i) > max_len9:
        max_len9 = len(i)
max_len9 =300
padded9 = np.array([i + [0]*(max_len9-len(i)) for i in tokenized9.values])

np.array(padded9).shape # Dimensions of the padded variable
#Masking
attention_mask9 = np.where(padded9 != 0, 1, 0)
attention_mask9.shape
input_ids9 = torch.tensor(padded9)  
attention_mask9 = torch.tensor(attention_mask9)

#Running the model () function through BERT
input_segments9= np.array([_get_segments3(token, max_len9)for token in tokenized9.values])
token_type_ids9 = torch.tensor(input_segments9)
input_segments9 = torch.tensor(input_segments9)

with torch.no_grad():
    last_hidden_states9 = model(input_ids9, attention_mask=attention_mask9, token_type_ids=input_segments9)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features9 = last_hidden_states9[0][:,0,:].numpy()
#features9

"""# df10"""

#Tokenization
pair10= df10['Title_1'] + df10['Content_1'] + [" [SEP] "] 
tokenized10 = pair10.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len10 = 0                 # padding all lists to the same size
for i in tokenized10.values:
    if len(i) > max_len10:
        max_len10 = len(i)
max_len10 =300
padded10 = np.array([i + [0]*(max_len10-len(i)) for i in tokenized10.values])

np.array(padded10).shape # Dimensions of the padded variable
#Masking
attention_mask10 = np.where(padded10 != 0, 1, 0)
attention_mask10.shape
input_ids10 = torch.tensor(padded10)  
attention_mask10 = torch.tensor(attention_mask10)

#Running the model () function through BERT
input_segments10 = np.array([_get_segments3(token, max_len10)for token in tokenized10.values])
token_type_ids10 = torch.tensor(input_segments10)
input_segments10 = torch.tensor(input_segments10)

with torch.no_grad():
    last_hidden_states10 = model(input_ids10, attention_mask=attention_mask10, token_type_ids=input_segments10)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features10 = last_hidden_states10[0][:,0,:].numpy()
#features10

"""# df11"""

#Tokenization
pair11= df11['Title_1'] + df11['Content_1'] + [" [SEP] "] 
tokenized11 = pair11.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len11 = 0                 # padding all lists to the same size
for i in tokenized11.values:
    if len(i) > max_len11:
        max_len11 = len(i)
max_len11 =300
padded11 = np.array([i + [0]*(max_len11-len(i)) for i in tokenized11.values])

np.array(padded11).shape # Dimensions of the padded variable
#Masking
attention_mask11 = np.where(padded11 != 0, 1, 0)
attention_mask11.shape
input_ids11 = torch.tensor(padded11)  
attention_mask11 = torch.tensor(attention_mask11)

#Running the model () function through BERT
input_segments11 = np.array([_get_segments3(token, max_len11)for token in tokenized11.values])
token_type_ids11 = torch.tensor(input_segments11)
input_segments11 = torch.tensor(input_segments11)

with torch.no_grad():
    last_hidden_states11 = model(input_ids11, attention_mask=attention_mask11, token_type_ids=input_segments11)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features11 = last_hidden_states11[0][:,0,:].numpy()
#features11

"""# df12"""

#Tokenization
pair12= df12['Title_1'] + df12['Content_1'] + [" [SEP] "] 
tokenized12 = pair12.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len12 = 0                 # padding all lists to the same size
for i in tokenized12.values:
    if len(i) > max_len12:
        max_len12 = len(i)
max_len12 =300
padded12 = np.array([i + [0]*(max_len12-len(i)) for i in tokenized12.values])

np.array(padded12).shape # Dimensions of the padded variable
#Masking
attention_mask12 = np.where(padded12 != 0, 1, 0)
attention_mask12.shape
input_ids12 = torch.tensor(padded12)  
attention_mask12 = torch.tensor(attention_mask12)

#Running the model () function through BERT
input_segments12 = np.array([_get_segments3(token, max_len12)for token in tokenized12.values])
token_type_ids12 = torch.tensor(input_segments12)
input_segments12 = torch.tensor(input_segments12)

with torch.no_grad():
    last_hidden_states12 = model(input_ids12, attention_mask=attention_mask12, token_type_ids=input_segments12)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features12 = last_hidden_states12[0][:,0,:].numpy()
#features12

"""# df13"""

#Tokenization
pair13= df13['Title_1'] + df13['Content_1'] + [" [SEP] "]  
tokenized13 = pair13.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len13 = 0                 # padding all lists to the same size
for i in tokenized13.values:
    if len(i) > max_len13:
        max_len13 = len(i)
max_len13 =300
padded13 = np.array([i + [0]*(max_len13-len(i)) for i in tokenized13.values])

np.array(padded13).shape # Dimensions of the padded variable
#Masking
attention_mask13 = np.where(padded13 != 0, 1, 0)
attention_mask13.shape
input_ids13 = torch.tensor(padded13)  
attention_mask13 = torch.tensor(attention_mask13)

#Running the model () function through BERT
input_segments13 = np.array([_get_segments3(token, max_len13)for token in tokenized13.values])
token_type_ids13 = torch.tensor(input_segments13)
input_segments13 = torch.tensor(input_segments13)

with torch.no_grad():
    last_hidden_states13 = model(input_ids13, attention_mask=attention_mask13, token_type_ids=input_segments13)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features13 = last_hidden_states13[0][:,0,:].numpy()
#features13

"""# df14"""

#Tokenization
pair14= df14['Title_1'] + df14['Content_1'] + [" [SEP] "] 
tokenized14 = pair14.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len14 = 0                 # padding all lists to the same size
for i in tokenized14.values:
    if len(i) > max_len14:
        max_len14 = len(i)
max_len14 =300
padded14 = np.array([i + [0]*(max_len14-len(i)) for i in tokenized14.values])

np.array(padded14).shape # Dimensions of the padded variable
#Masking
attention_mask14 = np.where(padded14 != 0, 1, 0)
attention_mask14.shape
input_ids14 = torch.tensor(padded14)  
attention_mask14 = torch.tensor(attention_mask14)

#Running the model () function through BERT
input_segments14 = np.array([_get_segments3(token, max_len14)for token in tokenized14.values])
token_type_ids14 = torch.tensor(input_segments14)
input_segments14 = torch.tensor(input_segments14)


with torch.no_grad():
    last_hidden_states14 = model(input_ids14, attention_mask=attention_mask14, token_type_ids=input_segments14)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features14 = last_hidden_states14[0][:,0,:].numpy()
#features14

"""# df15"""

#Tokenization
pair15= df15['Title_1'] + df15['Content_1'] + [" [SEP] "]
tokenized15 = pair15.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len15 = 0                 # padding all lists to the same size
for i in tokenized15.values:
    if len(i) > max_len15:
        max_len15 = len(i)
max_len15 =300
padded15 = np.array([i + [0]*(max_len15-len(i)) for i in tokenized15.values])

np.array(padded15).shape # Dimensions of the padded variable
#Masking
attention_mask15 = np.where(padded15 != 0, 1, 0)
attention_mask15.shape
input_ids15 = torch.tensor(padded15)  
attention_mask15 = torch.tensor(attention_mask15)

#Running the model () function through BERT
input_segments15 = np.array([_get_segments3(token, max_len15)for token in tokenized15.values])
token_type_ids15 = torch.tensor(input_segments15)
input_segments15 = torch.tensor(input_segments15)

with torch.no_grad():
    last_hidden_states15 = model(input_ids15, attention_mask=attention_mask15, token_type_ids=input_segments15)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features15 = last_hidden_states15[0][:,0,:].numpy()
#features15

"""# df16"""

#Tokenization
pair16 = df16['Title_1'] + df16['Content_1'] + [" [SEP] "]
tokenized16 = pair16.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len16 = 0                 # padding all lists to the same size
for i in tokenized16.values:
    if len(i) > max_len16:
        max_len16 = len(i)
max_len16 =300
padded16 = np.array([i + [0]*(max_len16-len(i)) for i in tokenized16.values])

np.array(padded16).shape # Dimensions of the padded variable
#Masking
attention_mask16 = np.where(padded16 != 0, 1, 0)
attention_mask16.shape
input_ids16 = torch.tensor(padded16)  
attention_mask16 = torch.tensor(attention_mask16)

#Running the model () function through BERT
input_segments16 = np.array([_get_segments3(token, max_len16)for token in tokenized16.values])
token_type_ids16 = torch.tensor(input_segments16)
input_segments16 = torch.tensor(input_segments16)

with torch.no_grad():
    last_hidden_states16 = model(input_ids16, attention_mask=attention_mask16, token_type_ids=input_segments16)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features16 = last_hidden_states16[0][:,0,:].numpy()
#features16

"""# df17"""

#Tokenization
pair17 = df17['Title_1'] + df17['Content_1'] + [" [SEP] "] 
tokenized17 = pair17.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len17 = 0                 # padding all lists to the same size
for i in tokenized17.values:
    if len(i) > max_len17:
        max_len17 = len(i)
max_len17 =300
padded17 = np.array([i + [0]*(max_len17-len(i)) for i in tokenized17.values])

np.array(padded17).shape # Dimensions of the padded variable
#Masking
attention_mask17 = np.where(padded17 != 0, 1, 0)
attention_mask17.shape
input_ids17 = torch.tensor(padded17)  
attention_mask17 = torch.tensor(attention_mask17)

#Running the model () function through BERT
input_segments17 = np.array([_get_segments3(token, max_len17)for token in tokenized17.values])
token_type_ids17 = torch.tensor(input_segments17)
input_segments17 = torch.tensor(input_segments17)

with torch.no_grad():
    last_hidden_states17 = model(input_ids17, attention_mask=attention_mask17, token_type_ids=input_segments17)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features17 = last_hidden_states17[0][:,0,:].numpy()
#features17

"""# ***cls_PR1***"""

featuresP1 =np.concatenate([features1,features2,features3,features4,features5,features6,features7,features8,features9,features10,features11,features12,features13,features14,features15,features16,features17])
#featuresP1

featuresP1.shape

"""# ***Pull_2***

# *df18*
"""

#Tokenization
pair18= df1['Title_2'] + df1['Content_2'] + [" [SEP] "] 
tokenized18 = pair18.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len18 = 0                 # padding all lists to the same size
for i in tokenized18.values:
    if len(i) > max_len18:
        max_len18 = len(i)
max_len18 =300
padded18 = np.array([i + [0]*(max_len18-len(i)) for i in tokenized18.values])

np.array(padded18).shape # Dimensions of the padded variable
#Masking
attention_mask18 = np.where(padded18 != 0, 1, 0)
attention_mask18.shape
input_ids18 = torch.tensor(padded18)  
attention_mask18 = torch.tensor(attention_mask18)

#Running the model () function through BERT
input_segments18 = np.array([_get_segments3(token, max_len18)for token in tokenized18.values])
token_type_ids18 = torch.tensor(input_segments18)
input_segments18 = torch.tensor(input_segments18)

with torch.no_grad():
    last_hidden_states18 = model(input_ids18, attention_mask=attention_mask18, token_type_ids=input_segments18)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features18 = last_hidden_states18[0][:,0,:].numpy()
#features18

"""# df19"""

#Tokenization
pair19= df2['Title_2'] + df2['Content_2'] + [" [SEP] "]
tokenized19 = pair19.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len19 = 0                 # padding all lists to the same size
for i in tokenized19.values:
    if len(i) > max_len19:
        max_len19 = len(i)
max_len19 =300
padded19 = np.array([i + [0]*(max_len19-len(i)) for i in tokenized19.values])

np.array(padded19).shape # Dimensions of the padded variable
#Masking
attention_mask19 = np.where(padded19 != 0, 1, 0)
attention_mask19.shape
input_ids19 = torch.tensor(padded19)  
attention_mask19 = torch.tensor(attention_mask19)

#Running the model () function through BERT
input_segments19 = np.array([_get_segments3(token, max_len19)for token in tokenized19.values])
token_type_ids19 = torch.tensor(input_segments19)
input_segments19 = torch.tensor(input_segments19)

with torch.no_grad():
    last_hidden_states19 = model(input_ids19, attention_mask=attention_mask19, token_type_ids=input_segments19)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features19 = last_hidden_states19[0][:,0,:].numpy()
#features19

"""# *df20*"""

#Tokenization
pair20= df3['Title_2'] + df3['Content_2'] + [" [SEP] "]
tokenized20 = pair20.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len20 = 0                 # padding all lists to the same size
for i in tokenized20.values:
    if len(i) > max_len20:
        max_len20 = len(i)
max_len20 =300
padded20 = np.array([i + [0]*(max_len20-len(i)) for i in tokenized20.values])

np.array(padded20).shape # Dimensions of the padded variable
#Masking
attention_mask20 = np.where(padded20 != 0, 1, 0)
attention_mask20.shape
input_ids20 = torch.tensor(padded20)  
attention_mask20 = torch.tensor(attention_mask20)

#Running the model () function through BERT
input_segments20 = np.array([_get_segments3(token, max_len20)for token in tokenized20.values])
token_type_ids20 = torch.tensor(input_segments20)
input_segments20 = torch.tensor(input_segments20)

with torch.no_grad():
    last_hidden_states20 = model(input_ids20, attention_mask=attention_mask20, token_type_ids=input_segments20)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features20 = last_hidden_states20[0][:,0,:].numpy()
#features20

"""# *df21*"""

#Tokenization
pair21= df4['Title_2'] + df4['Content_2'] + [" [SEP] "]
tokenized21 = pair21.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len21 = 0                 # padding all lists to the same size
for i in tokenized21.values:
    if len(i) > max_len21:
        max_len21 = len(i)
max_len21 =300
padded21 = np.array([i + [0]*(max_len21-len(i)) for i in tokenized21.values])

np.array(padded21).shape # Dimensions of the padded variable
#Masking
attention_mask21 = np.where(padded21 != 0, 1, 0)
attention_mask21.shape
input_ids21 = torch.tensor(padded21)  
attention_mask21 = torch.tensor(attention_mask21)

#Running the model () function through BERT
input_segments21 = np.array([_get_segments3(token, max_len21)for token in tokenized21.values])
token_type_ids21 = torch.tensor(input_segments21)
input_segments21 = torch.tensor(input_segments21)

with torch.no_grad():
    last_hidden_states21 = model(input_ids21, attention_mask=attention_mask21, token_type_ids=input_segments21)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features21 = last_hidden_states21[0][:,0,:].numpy()
#features21

"""# *df22*"""

#Tokenization
pair22 = df5['Title_2'] + df5['Content_2'] + [" [SEP] "]
tokenized22 = pair22.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len22 = 0                 # padding all lists to the same size
for i in tokenized22.values:
    if len(i) > max_len22:
        max_len22 = len(i)
max_len22 =300
padded22 = np.array([i + [0]*(max_len22-len(i)) for i in tokenized22.values])

np.array(padded22).shape # Dimensions of the padded variable
#Masking
attention_mask22 = np.where(padded22 != 0, 1, 0)
attention_mask22.shape
input_ids22 = torch.tensor(padded22)  
attention_mask22 = torch.tensor(attention_mask22)

#Running the model () function through BERT
input_segments22 = np.array([_get_segments3(token, max_len22)for token in tokenized22.values])
token_type_ids22 = torch.tensor(input_segments22)
input_segments22 = torch.tensor(input_segments22)

with torch.no_grad():
    last_hidden_states22 = model(input_ids22, attention_mask=attention_mask22, token_type_ids=input_segments22)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features22 = last_hidden_states22[0][:,0,:].numpy()
#features22

"""# *df23*"""

#Tokenization
pair23= df6['Title_2'] + df6['Content_2'] + [" [SEP] "]
tokenized23 = pair23.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len23 = 0                 # padding all lists to the same size
for i in tokenized23.values:
    if len(i) > max_len23:
        max_len23 = len(i)
max_len23 =300
padded23 = np.array([i + [0]*(max_len23-len(i)) for i in tokenized23.values])

np.array(padded23).shape # Dimensions of the padded variable
#Masking
attention_mask23 = np.where(padded23 != 0, 1, 0)
attention_mask23.shape
input_ids23 = torch.tensor(padded23)  
attention_mask23 = torch.tensor(attention_mask23)

#Running the model () function through BERT
input_segments23 = np.array([_get_segments3(token, max_len23)for token in tokenized23.values])
token_type_ids23 = torch.tensor(input_segments23)
input_segments23 = torch.tensor(input_segments23)

with torch.no_grad():
    last_hidden_states23 = model(input_ids23, attention_mask=attention_mask23, token_type_ids=input_segments23)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features23 = last_hidden_states23[0][:,0,:].numpy()
#features23

"""# *df24*"""

#Tokenization
pair24 = df7['Title_2'] + df7['Content_2'] + [" [SEP] "]
tokenized24 = pair24.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len24 = 0                 # padding all lists to the same size
for i in tokenized24.values:
    if len(i) > max_len24:
        max_len24 = len(i)
max_len24 =300
padded24 = np.array([i + [0]*(max_len24-len(i)) for i in tokenized24.values])

np.array(padded24).shape # Dimensions of the padded variable
#Masking
attention_mask24 = np.where(padded24 != 0, 1, 0)
attention_mask24.shape
input_ids24 = torch.tensor(padded24)  
attention_mask24 = torch.tensor(attention_mask24)

#Running the model () function through BERT
input_segments24 = np.array([_get_segments3(token, max_len24)for token in tokenized24.values])
token_type_ids24 = torch.tensor(input_segments24)
input_segments24 = torch.tensor(input_segments24)

with torch.no_grad():
    last_hidden_states24 = model(input_ids24, attention_mask=attention_mask24, token_type_ids=input_segments24)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features24 = last_hidden_states24[0][:,0,:].numpy()
#features24

"""# *df25*"""

#Tokenization
pair25= df8['Title_2'] + df8['Content_2'] + [" [SEP] "]
tokenized25 = pair25.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len25 = 0                 # padding all lists to the same size
for i in tokenized25.values:
    if len(i) > max_len25:
        max_len25 = len(i)
max_len25 =300
padded25 = np.array([i + [0]*(max_len25-len(i)) for i in tokenized25.values])

np.array(padded25).shape # Dimensions of the padded variable
#Masking
attention_mask25 = np.where(padded25 != 0, 1, 0)
attention_mask25.shape
input_ids25 = torch.tensor(padded25)  
attention_mask25 = torch.tensor(attention_mask25)

#Running the model () function through BERT
input_segments25 = np.array([_get_segments3(token, max_len25)for token in tokenized25.values])
token_type_ids25 = torch.tensor(input_segments25)
input_segments25 = torch.tensor(input_segments25)

with torch.no_grad():
    last_hidden_states25 = model(input_ids25, attention_mask=attention_mask25, token_type_ids=input_segments25)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features25 = last_hidden_states25[0][:,0,:].numpy()
#features25

"""# *df26*"""

#Tokenization
pair26= df9['Title_2'] + df9['Content_2'] + [" [SEP] "]
tokenized26 = pair26.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len26 = 0                 # padding all lists to the same size
for i in tokenized26.values:
    if len(i) > max_len26:
        max_len26 = len(i)
max_len26 =300
padded26 = np.array([i + [0]*(max_len26-len(i)) for i in tokenized26.values])

np.array(padded26).shape # Dimensions of the padded variable
#Masking
attention_mask26 = np.where(padded26 != 0, 1, 0)
attention_mask26.shape
input_ids26 = torch.tensor(padded26)  
attention_mask26 = torch.tensor(attention_mask26)

#Running the model () function through BERT
input_segments26= np.array([_get_segments3(token, max_len26)for token in tokenized26.values])
token_type_ids26 = torch.tensor(input_segments26)
input_segments26 = torch.tensor(input_segments26)

with torch.no_grad():
    last_hidden_states26 = model(input_ids26, attention_mask=attention_mask26, token_type_ids=input_segments26)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features26 = last_hidden_states26[0][:,0,:].numpy()
#features26

"""# df27"""

#Tokenization
pair27 = df10['Title_2'] + df10['Content_2'] + [" [SEP] "]
tokenized27 = pair27.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len27 = 0                 # padding all lists to the same size
for i in tokenized27.values:
    if len(i) > max_len27:
        max_len27 = len(i)
max_len27 =300
padded27 = np.array([i + [0]*(max_len27-len(i)) for i in tokenized27.values])

np.array(padded27).shape # Dimensions of the padded variable
#Masking
attention_mask27 = np.where(padded27 != 0, 1, 0)
attention_mask27.shape
input_ids27 = torch.tensor(padded27)  
attention_mask27 = torch.tensor(attention_mask27)

#Running the model () function through BERT
input_segments27= np.array([_get_segments3(token, max_len27)for token in tokenized27.values])
token_type_ids27 = torch.tensor(input_segments27)
input_segments27 = torch.tensor(input_segments27)

with torch.no_grad():
    last_hidden_states27 = model(input_ids27, attention_mask=attention_mask27, token_type_ids=input_segments27)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features27 = last_hidden_states27[0][:,0,:].numpy()
#features27

"""# df28"""

#Tokenization
pair28 = df11['Title_2'] + df11['Content_2'] + [" [SEP] "]
tokenized28 = pair28.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len28 = 0                 # padding all lists to the same size
for i in tokenized28.values:
    if len(i) > max_len28:
        max_len28 = len(i)
max_len28 =300
padded28 = np.array([i + [0]*(max_len28-len(i)) for i in tokenized28.values])

np.array(padded27).shape # Dimensions of the padded variable
#Masking
attention_mask28 = np.where(padded28 != 0, 1, 0)
attention_mask28.shape
input_ids28 = torch.tensor(padded28)  
attention_mask28 = torch.tensor(attention_mask28)

#Running the model () function through BERT
input_segments28 = np.array([_get_segments3(token, max_len28)for token in tokenized28.values])
token_type_ids28 = torch.tensor(input_segments28)
input_segments28 = torch.tensor(input_segments28)

with torch.no_grad():
    last_hidden_states28 = model(input_ids28, attention_mask=attention_mask28, token_type_ids=input_segments28)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features28 = last_hidden_states28[0][:,0,:].numpy()
#features28

"""# df29"""

#Tokenization
pair29 = df12['Title_2'] + df12['Content_2'] + [" [SEP] "]
tokenized29 = pair29.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len29 = 0                 # padding all lists to the same size
for i in tokenized29.values:
    if len(i) > max_len29:
        max_len29 = len(i)
max_len29 =300
padded29 = np.array([i + [0]*(max_len29-len(i)) for i in tokenized29.values])

np.array(padded29).shape # Dimensions of the padded variable
#Masking
attention_mask29 = np.where(padded29 != 0, 1, 0)
attention_mask29.shape
input_ids29 = torch.tensor(padded29)  
attention_mask29 = torch.tensor(attention_mask29)

#Running the model () function through BERT
input_segments29 = np.array([_get_segments3(token, max_len29)for token in tokenized29.values])
token_type_ids29 = torch.tensor(input_segments29)
input_segments29 = torch.tensor(input_segments29)

with torch.no_grad():
    last_hidden_states29 = model(input_ids29, attention_mask=attention_mask29, token_type_ids=input_segments29)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features29 = last_hidden_states29[0][:,0,:].numpy()
#features29

"""# df30"""

#Tokenization
pair30 = df13['Title_2'] + df13['Content_2'] + [" [SEP] "]
tokenized30 = pair30.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len30 = 0                 # padding all lists to the same size
for i in tokenized30.values:
    if len(i) > max_len30:
        max_len30 = len(i)
max_len30 =300
padded30 = np.array([i + [0]*(max_len30-len(i)) for i in tokenized30.values])

np.array(padded30).shape # Dimensions of the padded variable
#Masking
attention_mask30 = np.where(padded30 != 0, 1, 0)
attention_mask30.shape
input_ids30 = torch.tensor(padded30)  
attention_mask30 = torch.tensor(attention_mask30)

#Running the model () function through BERT
input_segments30 = np.array([_get_segments3(token, max_len30)for token in tokenized30.values])
token_type_ids30 = torch.tensor(input_segments30)
input_segments30 = torch.tensor(input_segments30)

with torch.no_grad():
    last_hidden_states30 = model(input_ids30, attention_mask=attention_mask30, token_type_ids=input_segments30)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features30 = last_hidden_states30[0][:,0,:].numpy()
#features30

"""# 31"""

#Tokenization
pair31 = df14['Title_2'] + df14['Content_2'] + [" [SEP] "]
tokenized31 = pair31.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len31 = 0                 # padding all lists to the same size
for i in tokenized31.values:
    if len(i) > max_len31:
        max_len31 = len(i)
max_len31 =300
padded31 = np.array([i + [0]*(max_len31-len(i)) for i in tokenized31.values])

np.array(padded31).shape # Dimensions of the padded variable
#Masking
attention_mask31 = np.where(padded31 != 0, 1, 0)
attention_mask31.shape
input_ids31 = torch.tensor(padded31)  
attention_mask31 = torch.tensor(attention_mask31)

#Running the model () function through BERT
input_segments31 = np.array([_get_segments3(token, max_len31)for token in tokenized31.values])
token_type_ids31 = torch.tensor(input_segments31)
input_segments31 = torch.tensor(input_segments31)

with torch.no_grad():
    last_hidden_states31 = model(input_ids31, attention_mask=attention_mask31, token_type_ids=input_segments31)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features31 = last_hidden_states31[0][:,0,:].numpy()
#features31

"""# df32"""

#Tokenization
pair32 = df15['Title_2'] + df15['Content_2'] + [" [SEP] "]
tokenized32 = pair32.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len32 = 0                 # padding all lists to the same size
for i in tokenized32.values:
    if len(i) > max_len32:
        max_len32 = len(i)
max_len32 =300
padded32 = np.array([i + [0]*(max_len32-len(i)) for i in tokenized32.values])

np.array(padded32).shape # Dimensions of the padded variable
#Masking
attention_mask32 = np.where(padded32 != 0, 1, 0)
attention_mask32.shape
input_ids32 = torch.tensor(padded32)  
attention_mask32 = torch.tensor(attention_mask32)

#Running the model () function through BERT
input_segments32 = np.array([_get_segments3(token, max_len32)for token in tokenized32.values])
token_type_ids32 = torch.tensor(input_segments32)
input_segments32 = torch.tensor(input_segments32)

with torch.no_grad():
    last_hidden_states32 = model(input_ids32, attention_mask=attention_mask32, token_type_ids=input_segments32)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features32 = last_hidden_states32[0][:,0,:].numpy()
#features32

"""# df33"""

#Tokenization
pair33 = df16['Title_2'] + df16['Content_2'] + [" [SEP] "]
tokenized33 = pair33.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len33 = 0                 # padding all lists to the same size
for i in tokenized33.values:
    if len(i) > max_len33:
        max_len33 = len(i)
max_len33 =300
padded33 = np.array([i + [0]*(max_len33-len(i)) for i in tokenized33.values])

np.array(padded33).shape # Dimensions of the padded variable
#Masking
attention_mask33 = np.where(padded33 != 0, 1, 0)
attention_mask33.shape
input_ids33 = torch.tensor(padded33)  
attention_mask33 = torch.tensor(attention_mask33)

#Running the model () function through BERT
input_segments33 = np.array([_get_segments3(token, max_len33)for token in tokenized33.values])
token_type_ids33 = torch.tensor(input_segments33)
input_segments33 = torch.tensor(input_segments33)

with torch.no_grad():
    last_hidden_states33 = model(input_ids33, attention_mask=attention_mask33, token_type_ids=input_segments33)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features33 = last_hidden_states33[0][:,0,:].numpy()
#features33

"""# df34"""

#Tokenization
pair34 = df17['Title_2'] + df17['Content_2'] + [" [SEP] "]
tokenized34 = pair34.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len34 = 0                 # padding all lists to the same size
for i in tokenized34.values:
    if len(i) > max_len34:
        max_len34 = len(i)
max_len34 =300
padded34 = np.array([i + [0]*(max_len34-len(i)) for i in tokenized34.values])

np.array(padded34).shape # Dimensions of the padded variable
#Masking
attention_mask34 = np.where(padded34 != 0, 1, 0)
attention_mask34.shape
input_ids34 = torch.tensor(padded34)  
attention_mask34 = torch.tensor(attention_mask34)

#Running the model () function through BERT
input_segments34 = np.array([_get_segments3(token, max_len34)for token in tokenized34.values])
token_type_ids34 = torch.tensor(input_segments34)
input_segments34 = torch.tensor(input_segments34)

with torch.no_grad():
    last_hidden_states34 = model(input_ids34, attention_mask=attention_mask34, token_type_ids=input_segments34)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features34 = last_hidden_states34[0][:,0,:].numpy()
#features34

"""# ***cls_PR2***"""

featuresP2=np.concatenate([features18,features19,features20,features21,features22,features23,features24,features25,features26,features27,features28,features29,features30,features31,features32,features33,features34])

#featuresP2

featuresP2.shape

"""# ***Max pooling***"""

featuresP1.shape

featuresP2.shape

import tensorflow as tf
from keras.models import Sequential
from keras.layers import MaxPooling1D

p1 = tf.constant([featuresP1])
p1 = tf.reshape(p1, [3328, 768, 1])
model = Sequential(
    [MaxPooling1D()])
outputp1 = model.predict(p1)
outputp1 = np.squeeze(outputp1)
#print(outputp1)

outputp1.shape

p2 = tf.constant([featuresP2])
p2 = tf.reshape(p1, [3328, 768, 1])
model = Sequential(
    [MaxPooling1D()])
outputp2 = model.predict(p2)
outputp2 = np.squeeze(outputp2)
#print(outputp2)

outputp2.shape

"""# ***Concatenation***"""

features =np.concatenate([outputp1,outputp2], axis=1)
#features

features.shape

"""# ***Classification***"""

Total = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17], ignore_index=True)

labels =Total['Label']
#labels

"""# ***Hold out***"""

#train_features,test_features,train_labels,test_labels= train_test_split(features,labels,test_size=0.2, random_state=2)
#print('Number of data points in train data:', train_features.shape[0])
#print('Number of data points in train data:', train_labels.shape[0])
#print('Number of data points in test data:', train_features.shape[0])
#print('Number of data points in test data:', train_labels.shape[0])
#print(features.shape,train_features.shape,test_features.shape)

train_features = features[:2662]
train_labels = labels[:2662]
test_features = features[2662:]
test_labels = labels[2662:]

import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(test_labels, y_predLr):
    C = confusion_matrix(test_labels, y_predLr)
    A =(((C.T)/(C.sum(axis=1))).T)
    B =(C/C.sum(axis=0))
    plt.figure(figsize=(20,4))
    labels = [0,1]
    plt.subplot(1, 3, 1)
    sns.heatmap(C, annot=True, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Confusion matrix")
    
    plt.subplot(1, 3, 2)
    sns.heatmap(B, annot=True,  fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Precision matrix")
    
    plt.subplot(1, 3, 3)
    # representing B in heatmap format
    sns.heatmap(A, annot=True, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Recall matrix")
    
    plt.show()

"""# ***LogisticRegression Optimized***"""

#n_splits=2
#cross_val_score=5
parameters = {'C': np.linspace(0.0001, 100, 20)}
grid_search = GridSearchCV(LogisticRegression(), parameters, cv=5)
grid_search.fit(train_features, train_labels)
print('best parameters: ', grid_search.best_params_)
print('best scrores: ', grid_search.best_score_)

lr_clf = LogisticRegression(C=10.52)
lr_clf.fit(train_features, train_labels)

lr_clf.score(test_features, test_labels)

y_predLr = lr_clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_predLr

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_predLr))

print(confusion_matrix(test_labels, y_predLr))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_predLr))

#plot_confusion_matrix(test_labels, y_predLr)

scores = cross_val_score(lr_clf, features, labels,cv=5)
print("mean: {:.3f} (std: {:.3f})".format(scores.mean(),
                                          scores.std()),
                                          end="\n\n" )

"""# ***Decision tree***"""

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth = 500, random_state = 0)
clf.fit(train_features, train_labels)

y_preddt = clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_preddt

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_preddt))

print(confusion_matrix(test_labels, y_preddt))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_preddt))

# The score method returns the accuracy of the model
score = clf.score(test_features, test_labels)
print(score)

#plot_confusion_matrix(test_labels, y_preddt)

"""# ***SVM***"""

from sklearn.svm import SVC
svclassifier = SVC(kernel='linear')
svclassifier.fit(train_features, train_labels)

y_pred = svclassifier.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_pred))

print(confusion_matrix(test_labels,y_pred))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_pred))

#plot_confusion_matrix(test_labels,y_pred)

"""# ***MLP***"""

from sklearn import datasets
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('ggplot')

model = MLPClassifier()
model.fit(train_features, train_labels)
print(model)

expected_y  = test_labels
predicted_y = model.predict(test_features)

print(metrics.classification_report(expected_y, predicted_y))

print(metrics.confusion_matrix(expected_y, predicted_y))

expected_y, predicted_y = test_labels , clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
predicted_y

#plot_confusion_matrix(expected_y, predicted_y)

"""# ***MLP Best params***"""

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(max_iter=100)
from sklearn.datasets import make_classification

parameter_space = {
    'hidden_layer_sizes': [(50,100,50), (50,100,50), (100,)],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}

from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)
clf.fit(train_features, train_labels)

# Best paramete set
print('Best parameters found:\n', clf.best_params_)

# All results
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

y_true, y_pred = test_labels , clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report, confusion_matrix
#print('Results on the test set:')
#print(classification_report(y_true, y_pred))

#print(confusion_matrix(y_true, y_pred))

y_true, y_pred = test_labels , clf.predict(test_features)

from sklearn.metrics import accuracy_score
#print(accuracy_score(y_true, y_pred))
clf.score(test_features, test_labels)

#plot_confusion_matrix(y_true, y_pred)

"""# ***Random Forest***"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=20, random_state=0)
rf.fit(train_features, train_labels)
y_pred1 = rf.predict(test_features)
y_pred1

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_pred1))

print(confusion_matrix(test_labels, y_pred1))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_pred1))

#plot_confusion_matrix(test_labels, y_pred1)

"""# ***Naive Bayes***"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(train_features, train_labels)

y_pred = gnb.predict(test_features)
y_pred

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(test_labels, y_pred))

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_pred))

print(confusion_matrix(test_labels, y_pred))

#plot_confusion_matrix(test_labels, y_pred)

"""# ***XGBoost***"""

import xgboost as xgb
modelxgb=xgb.XGBClassifier(random_state=1,learning_rate=0.01)
modelxgb.fit(train_features, train_labels)

predxgb = modelxgb.predict(test_features)
predxgb

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,predxgb))

print(confusion_matrix(test_labels, predxgb))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, predxgb))
modelxgb.score(test_features,test_labels)

#plot_confusion_matrix(test_labels, predxgb)

"""# ***KNN***"""

#import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
#Setup arrays to store training and test accuracies
neighbors = np.arange(1,9)
train_accuracy =np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

for i,k in enumerate(neighbors):
    #Setup a knn classifier with k neighbors
    knn = KNeighborsClassifier(n_neighbors=k)
    
    #Fit the model
    knn.fit(train_features, train_labels)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(train_features, train_labels)
    
    #Compute accuracy on the test set
    test_accuracy[i] = knn.score(test_features, test_labels)

import matplotlib.pyplot as plt
plt.style.use('ggplot')
#Generate plot
plt.title('k-NN Varying number of neighbors')
plt.plot(neighbors, test_accuracy, label='Testing Accuracy')
plt.plot(neighbors, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
plt.show()

knn = KNeighborsClassifier(n_neighbors=7)
#Fit the model
knn.fit(train_features,train_labels)

knn.score(test_features,test_labels)

y_pred = knn.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report
print(classification_report(test_labels,y_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(test_labels,y_pred))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_pred))

#plot_confusion_matrix(test_labels,y_pred)