# -*- coding: utf-8 -*-
"""VR4_Bert_3329_RANIA_Title.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QjJEo7KNIHOMZuakBfVX8Y6FzvDqDq0y
"""

!pip install transformers

"""# ***Importing the tools***"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import torch
import transformers as ppb
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import warnings
import re
warnings.filterwarnings('ignore')

"""# ***Importing the dataset from Drive***"""

from google.colab import drive 
drive.mount('/content/gdrive')

#df=pd.read_csv('/content/gdrive/MyDrive/BERT_Final_3329.csv', delimiter=';', encoding='cp437')
df=pd.read_csv('/content/gdrive/MyDrive/VR_4/V4_DATA.csv', delimiter=';', encoding='cp437')
df

#lire la base des données ligne par ligne 
pd.set_option('display.max_rows',df.shape[0]+1)
df

#voir les valeurs manquantes
df.isnull().sum().sort_values(ascending=False)

"""# ***Loading the Pre-trained BERT model***"""

import time
start = time.time()

model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')
tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
model = model_class.from_pretrained(pretrained_weights)

"""# ***Remove stop words***"""

df['Title_1']= df['Title_1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

df['Title_2']= df['Title_2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

df['Content_1']= df['Content_1'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

df['Content_2']= df['Content_2'].str.replace(r'i' 'me' 'my' 'myself' 'we' 
                                         'our' 'ours' 'ourselves' 'you' 'your' 'yours' 'yourself''yourselves' 'they' 'we' 'him' 'he' 'him' 'his' 'himself' 'she' 'her' 'hers' 'herself' 'it''its' 'itself' 'they' 'them' 'their' 'theirs' 'themselves'  'what' 'which' 'who' 'whom' 'this' 'that'
                                         'these' 'those' 'am' 'is' 'are' 'was' 'were' 'be' 'been' 'being' 'have' 'has' 'had' 'having' 'do' 'does' 'did' 'doing' 'a' 'an' 'the' 'and' 'but' 'if' 'or' 'because' 'as' 'until' 'while' 'of' 'at' 'by' 'for' 'with' 'about' 'against' 'between' 'into' 'through' 
                                         'during' 'before' 'after' 'above' 'below' 'to' 'from' 'up' 'down' 'in' 'out' 'on' 'off' 'over' 'under' 'again' 'further' 'then' 'once'  'here' 'there' 'when' 'where' 'why' 'how' 'all' 'any' 'both' 'each' 'few' 'more' 'most' 'other' 'some' 'such' 'no' 'nor' 'not' 'only' 'own' 'same' 'so' 'than' 'too' 'very' 's' 't' 'can' 'will' 'just' 'don' 'should' 'now'
                                         'java' 'com' 'org' ,'')

"""# ***Remove Digits:***"""

df['Title_1']= df['Title_1'].str.replace(r'0', '')
df['Title_1']= df['Title_1'].str.replace(r'1', '')
df['Title_1']= df['Title_1'].str.replace(r'2', '')
df['Title_1']= df['Title_1'].str.replace(r'3', '')
df['Title_1']= df['Title_1'].str.replace(r'4', '')
df['Title_1']= df['Title_1'].str.replace(r'5', '')
df['Title_1']= df['Title_1'].str.replace(r'6', '')
df['Title_1']= df['Title_1'].str.replace(r'7', '')
df['Title_1']= df['Title_1'].str.replace(r'8', '')
df['Title_1']= df['Title_1'].str.replace(r'9', '')

df['Title_2']= df['Title_2'].str.replace(r'0', '')
df['Title_2']= df['Title_2'].str.replace(r'1', '')
df['Title_2']= df['Title_2'].str.replace(r'2', '')
df['Title_2']= df['Title_2'].str.replace(r'3', '')
df['Title_2']= df['Title_2'].str.replace(r'4', '')
df['Title_2']= df['Title_2'].str.replace(r'5', '')
df['Title_2']= df['Title_2'].str.replace(r'6', '')
df['Title_2']= df['Title_2'].str.replace(r'7', '')
df['Title_2']= df['Title_2'].str.replace(r'8', '')
df['Title_2']= df['Title_2'].str.replace(r'9', '')

df['Content_1']= df['Content_1'].str.replace(r'0', '')
df['Content_1']= df['Content_1'].str.replace(r'1', '')
df['Content_1']= df['Content_1'].str.replace(r'2', '')
df['Content_1']= df['Content_1'].str.replace(r'3', '')
df['Content_1']= df['Content_1'].str.replace(r'4', '')
df['Content_1']= df['Content_1'].str.replace(r'5', '')
df['Content_1']= df['Content_1'].str.replace(r'6', '')
df['Content_1']= df['Content_1'].str.replace(r'7', '')
df['Content_1']= df['Content_1'].str.replace(r'8', '')
df['Content_1']= df['Content_1'].str.replace(r'9', '')

df['Content_2']= df['Content_2'].str.replace(r'0', '')
df['Content_2']= df['Content_2'].str.replace(r'1', '')
df['Content_2']= df['Content_2'].str.replace(r'2', '')
df['Content_2']= df['Content_2'].str.replace(r'3', '')
df['Content_2']= df['Content_2'].str.replace(r'4', '')
df['Content_2']= df['Content_2'].str.replace(r'5', '')
df['Content_2']= df['Content_2'].str.replace(r'6', '')
df['Content_2']= df['Content_2'].str.replace(r'7', '')
df['Content_2']= df['Content_2'].str.replace(r'8', '')
df['Content_2']= df['Content_2'].str.replace(r'9', '')

"""# ***Remove special characters***"""

df['Title_1']= df['Title_1'].str.replace(r'/', '')
df['Title_1']= df['Title_1'].str.replace(r'@', '')
df['Title_1']= df['Title_1'].str.replace(r'!', '')
df['Title_1']= df['Title_1'].str.replace(r'+', '')
df['Title_1']= df['Title_1'].str.replace(r'-', '')
df['Title_1']= df['Title_1'].str.replace(r'/', '')
df['Title_1']= df['Title_1'].str.replace(r':', '')
df['Title_1']= df['Title_1'].str.replace(r';', '')
df['Title_1']= df['Title_1'].str.replace(r'>', '')
df['Title_1']= df['Title_1'].str.replace(r'=', '')
df['Title_1']= df['Title_1'].str.replace(r'<', '')
df['Title_1']= df['Title_1'].str.replace(r'(', '')
df['Title_1']= df['Title_1'].str.replace(r')', '')
df['Title_1']= df['Title_1'].str.replace(r'#', '')
df['Title_1']= df['Title_1'].str.replace(r'$', '')
df['Title_1']= df['Title_1'].str.replace(r'*', '')
df['Title_1']= df['Title_1'].str.replace(r'_', '')
df['Title_1']= df['Title_1'].str.replace(r']', '')
df['Title_1']= df['Title_1'].str.replace(r'[', '')
df['Title_1']= df['Title_1'].str.replace(r'{', '')
df['Title_1']= df['Title_1'].str.replace(r'}', '')
df['Title_1']= df['Title_1'].str.replace(r'"', '')
df['Title_1']= df['Title_1'].str.replace(r'&', '')
df['Title_1']= df['Title_1'].str.replace(r'~', '')
df['Title_1']= df['Title_1'].str.replace(r'^', '')
df['Title_1']= df['Title_1'].str.replace(r'°', '')
df['Title_1']= df['Title_1'].str.replace(r'?', '')
df['Title_1']= df['Title_1'].str.replace(r'%', '')

df['Title_2']= df['Title_2'].str.replace(r'/', '')
df['Title_2']= df['Title_2'].str.replace(r'@', '')
df['Title_2']= df['Title_2'].str.replace(r'!', '')
df['Title_2']= df['Title_2'].str.replace(r'+', '')
df['Title_2']= df['Title_2'].str.replace(r'-', '')
df['Title_2']= df['Title_2'].str.replace(r'/', '')
df['Title_2']= df['Title_2'].str.replace(r':', '')
df['Title_2']= df['Title_2'].str.replace(r';', '')
df['Title_2']= df['Title_2'].str.replace(r'>', '')
df['Title_2']= df['Title_2'].str.replace(r'=', '')
df['Title_2']= df['Title_2'].str.replace(r'<', '')
df['Title_2']= df['Title_2'].str.replace(r'(', '')
df['Title_2']= df['Title_2'].str.replace(r')', '')
df['Title_2']= df['Title_2'].str.replace(r'#', '')
df['Title_2']= df['Title_2'].str.replace(r'$', '')
df['Title_2']= df['Title_2'].str.replace(r'*', '')
df['Title_2']= df['Title_2'].str.replace(r'_', '')
df['Title_2']= df['Title_2'].str.replace(r']', '')
df['Title_2']= df['Title_2'].str.replace(r'[', '')
df['Title_2']= df['Title_2'].str.replace(r'{', '')
df['Title_2']= df['Title_2'].str.replace(r'}', '')
df['Title_2']= df['Title_2'].str.replace(r'"', '')
df['Title_2']= df['Title_2'].str.replace(r'&', '')
df['Title_2']= df['Title_2'].str.replace(r'~', '')
df['Title_2']= df['Title_2'].str.replace(r'^', '')
df['Title_2']= df['Title_2'].str.replace(r'°', '')
df['Title_2']= df['Title_2'].str.replace(r'?', '')
df['Title_2']= df['Title_2'].str.replace(r'%', '')

df['Content_1']= df['Content_1'].str.replace(r'/', '')
df['Content_1']= df['Content_1'].str.replace(r'@', '')
df['Content_1']= df['Content_1'].str.replace(r'!', '')
df['Content_1']= df['Content_1'].str.replace(r'+', '')
df['Content_1']= df['Content_1'].str.replace(r'-', '')
df['Content_1']= df['Content_1'].str.replace(r'/', '')
df['Content_1']= df['Content_1'].str.replace(r':', '')
df['Content_1']= df['Content_1'].str.replace(r';', '')
df['Content_1']= df['Content_1'].str.replace(r'>', '')
df['Content_1']= df['Content_1'].str.replace(r'=', '')
df['Content_1']= df['Content_1'].str.replace(r'<', '')
df['Content_1']= df['Content_1'].str.replace(r'(', '')
df['Content_1']= df['Content_1'].str.replace(r')', '')
df['Content_1']= df['Content_1'].str.replace(r'#', '')
df['Content_1']= df['Content_1'].str.replace(r'$', '')
df['Content_1']= df['Content_1'].str.replace(r'*', '')
df['Content_1']= df['Content_1'].str.replace(r'_', '')
df['Content_1']= df['Content_1'].str.replace(r']', '')
df['Content_1']= df['Content_1'].str.replace(r'[', '')
df['Content_1']= df['Content_1'].str.replace(r'{', '')
df['Content_1']= df['Content_1'].str.replace(r'}', '')
df['Content_1']= df['Content_1'].str.replace(r'"', '')
df['Content_1']= df['Content_1'].str.replace(r'&', '')
df['Content_1']= df['Content_1'].str.replace(r'~', '')
df['Content_1']= df['Content_1'].str.replace(r'^', '')
df['Content_1']= df['Content_1'].str.replace(r'°', '')
df['Content_1']= df['Content_1'].str.replace(r'?', '')
df['Content_1']= df['Content_1'].str.replace(r'%', '')

df['Content_2']= df['Content_2'].str.replace(r'/', '')
df['Content_2']= df['Content_2'].str.replace(r'@', '')
df['Content_2']= df['Content_2'].str.replace(r'!', '')
df['Content_2']= df['Content_2'].str.replace(r'+', '')
df['Content_2']= df['Content_2'].str.replace(r'-', '')
df['Content_2']= df['Content_2'].str.replace(r'/', '')
df['Content_2']= df['Content_2'].str.replace(r':', '')
df['Content_2']= df['Content_2'].str.replace(r';', '')
df['Content_2']= df['Content_2'].str.replace(r'>', '')
df['Content_2']= df['Content_2'].str.replace(r'=', '')
df['Content_2']= df['Content_2'].str.replace(r'<', '')
df['Content_2']= df['Content_2'].str.replace(r'(', '')
df['Content_2']= df['Content_2'].str.replace(r')', '')
df['Content_2']= df['Content_2'].str.replace(r'#', '')
df['Content_2']= df['Content_2'].str.replace(r'$', '')
df['Content_2']= df['Content_2'].str.replace(r'*', '')
df['Content_2']= df['Content_2'].str.replace(r'_', '')
df['Content_2']= df['Content_2'].str.replace(r']', '')
df['Content_2']= df['Content_2'].str.replace(r'[', '')
df['Content_2']= df['Content_2'].str.replace(r'{', '')
df['Content_2']= df['Content_2'].str.replace(r'}', '')
df['Content_2']= df['Content_2'].str.replace(r'"', '')
df['Content_2']= df['Content_2'].str.replace(r'&', '')
df['Content_2']= df['Content_2'].str.replace(r'~', '')
df['Content_2']= df['Content_2'].str.replace(r'^', '')
df['Content_2']= df['Content_2'].str.replace(r'°', '')
df['Content_2']= df['Content_2'].str.replace(r'?', '')
df['Content_2']= df['Content_2'].str.replace(r'%', '')

#lire la base des données ligne par ligne 
pd.set_option('display.max_rows',df.shape[0]+1)
df

"""# ***Batch ThunderBird***"""

df1=df[:200]
df2=df[200:400]
df3=df[400:600]
df4=df[600:800]
df5=df[800:1000]
df6=df[1000:1200]
df7=df[1200:1400]
df8=df[1400:1600]
df9=df[1600:1800]
df10=df[1800:2000]
df11=df[2000:2200]
df12=df[2200:2400]
df13=df[2400:2600]
df14=df[2600:2800]
df15=df[2800:3000]
df16=df[3000:3200]
df17=df[3200:]

"""# ***Function: _get_segments***"""

def _get_segments3(tokens, max_seq_length):
    """Segments: 0 for the first sequence, 1 for the second"""
    if len(tokens)>max_seq_length:
        raise IndexError("Token length more than max seq length!")
    segments = []
    first_sep = False
    current_segment_id = 0 
    for token in tokens:
        segments.append(current_segment_id)
        #print(token)
        if token == 102:
            #if first_sep:
                #first_sep = False 
            #else:
           current_segment_id = 1
    return segments + [0] * (max_seq_length - len(tokens))

"""# **df1**"""

#Tokenization
pair1= df1['Title_1'] + [" [SEP] "] + df1['Title_2'] 
tokenized1 = pair1.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len1 = 0                 # padding all lists to the same size
for i in tokenized1.values:
    if len(i) > max_len1:
        max_len2 = len(i)
max_len1 =300
padded1 = np.array([i + [0]*(max_len1-len(i)) for i in tokenized1.values])

np.array(padded1).shape # Dimensions of the padded variable
#Masking
attention_mask1 = np.where(padded1 != 0, 1, 0)
attention_mask1.shape
input_ids1 = torch.tensor(padded1)  
attention_mask1 = torch.tensor(attention_mask1)

#Running the model () function through BERT
input_segments1= np.array([_get_segments3(token, max_len1)for token in tokenized1.values])
token_type_ids1 = torch.tensor(input_segments1)
input_segments1 = torch.tensor(input_segments1)

with torch.no_grad():
    last_hidden_states1 = model(input_ids1, attention_mask=attention_mask1, token_type_ids=input_segments1)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features1 = last_hidden_states1[0][:,0,:].numpy()
features1

"""# ***df2***"""

#Tokenization
pair2= df2['Title_1'] + [" [SEP] "] + df2['Title_2']  
tokenized2 = pair2.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len2 = 0                 # padding all lists to the same size
for i in tokenized2.values:
    if len(i) > max_len2:
        max_len2 = len(i)
max_len2 =300
padded2 = np.array([i + [0]*(max_len2-len(i)) for i in tokenized2.values])

np.array(padded2).shape # Dimensions of the padded variable
#Masking
attention_mask2 = np.where(padded2 != 0, 1, 0)
attention_mask2.shape
input_ids2 = torch.tensor(padded2)  
attention_mask2 = torch.tensor(attention_mask2)

#Running the model () function through BERT
input_segments2= np.array([_get_segments3(token, max_len2)for token in tokenized2.values])
token_type_ids2 = torch.tensor(input_segments2)
input_segments2 = torch.tensor(input_segments2)

with torch.no_grad():
    last_hidden_states2 = model(input_ids2, attention_mask=attention_mask2, token_type_ids=input_segments2)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features2 = last_hidden_states2[0][:,0,:].numpy()
features2

"""# ***df3***"""

#Tokenization
pair3= df3['Title_1'] +  [" [SEP] "] + df3['Title_2']  
tokenized3 = pair3.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len3 = 0                 # padding all lists to the same size
for i in tokenized3.values:
    if len(i) > max_len3:
        max_len3 = len(i)
max_len3 =300
padded3 = np.array([i + [0]*(max_len3-len(i)) for i in tokenized3.values])

np.array(padded3).shape # Dimensions of the padded variable
#Masking
attention_mask3 = np.where(padded3 != 0, 1, 0)
attention_mask3.shape
input_ids3 = torch.tensor(padded3)  
attention_mask3 = torch.tensor(attention_mask3)

#Running the model () function through BERT
input_segments3= np.array([_get_segments3(token, max_len3)for token in tokenized3.values])
token_type_ids3 = torch.tensor(input_segments3)
input_segments3 = torch.tensor(input_segments3)

with torch.no_grad():
    last_hidden_states3 = model(input_ids3, attention_mask=attention_mask3, token_type_ids=input_segments3)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features3 = last_hidden_states3[0][:,0,:].numpy()
features3

"""# ***df4***"""

#Tokenization
pair4= df4['Title_1'] +  [" [SEP] "] + df4['Title_2'] 
tokenized4 = pair4.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len4 = 0                 # padding all lists to the same size
for i in tokenized4.values:
    if len(i) > max_len4:
        max_len4 = len(i)
max_len4 =300
padded4 = np.array([i + [0]*(max_len4-len(i)) for i in tokenized4.values])

np.array(padded4).shape # Dimensions of the padded variable
#Masking
attention_mask4 = np.where(padded4 != 0, 1, 0)
attention_mask4.shape
input_ids4 = torch.tensor(padded4)  
attention_mask4 = torch.tensor(attention_mask4)

#Running the model () function through BERT
input_segments4= np.array([_get_segments3(token, max_len4)for token in tokenized4.values])
token_type_ids4 = torch.tensor(input_segments4)
input_segments4 = torch.tensor(input_segments4)

with torch.no_grad():
    last_hidden_states4 = model(input_ids4, attention_mask=attention_mask4, token_type_ids=input_segments4)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features4 = last_hidden_states4[0][:,0,:].numpy()
features4

"""# ***df5***"""

#Tokenization
pair5= df5['Title_1'] + [" [SEP] "] + df5['Title_2']  
tokenized5 = pair5.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len5 = 0                 # padding all lists to the same size
for i in tokenized5.values:
    if len(i) > max_len5:
        max_len5 = len(i)
max_len5 =300
padded5 = np.array([i + [0]*(max_len5-len(i)) for i in tokenized5.values])

np.array(padded5).shape # Dimensions of the padded variable
#Masking
attention_mask5 = np.where(padded5 != 0, 1, 0)
attention_mask5.shape
input_ids5 = torch.tensor(padded5)  
attention_mask5 = torch.tensor(attention_mask5)

#Running the model () function through BERT
input_segments5= np.array([_get_segments3(token, max_len5)for token in tokenized5.values])
token_type_ids5 = torch.tensor(input_segments5)
input_segments5 = torch.tensor(input_segments5)

with torch.no_grad():
    last_hidden_states5 = model(input_ids5, attention_mask=attention_mask5, token_type_ids=input_segments5)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features5 = last_hidden_states5[0][:,0,:].numpy()
features5

"""# ***df6***"""

#Tokenization
pair6= df6['Title_1'] +  [" [SEP] "] + df6['Title_2'] 
tokenized6 = pair6.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len6 = 0                 # padding all lists to the same size
for i in tokenized6.values:
    if len(i) > max_len6:
        max_len6 = len(i)
max_len6 =300
padded6 = np.array([i + [0]*(max_len6-len(i)) for i in tokenized6.values])

np.array(padded6).shape # Dimensions of the padded variable
#Masking
attention_mask6 = np.where(padded6 != 0, 1, 0)
attention_mask6.shape
input_ids6 = torch.tensor(padded6)  
attention_mask6 = torch.tensor(attention_mask6)

#Running the model () function through BERT
input_segments6= np.array([_get_segments3(token, max_len6)for token in tokenized6.values])
token_type_ids6 = torch.tensor(input_segments6)
input_segments6 = torch.tensor(input_segments6)

with torch.no_grad():
    last_hidden_states6 = model(input_ids6, attention_mask=attention_mask6, token_type_ids=input_segments6)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features6 = last_hidden_states6[0][:,0,:].numpy()
features6

"""# **df7**"""

#Tokenization
pair7= df7['Title_1'] +  [" [SEP] "] + df7['Title_2'] 
tokenized7 = pair7.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len7 = 0                 # padding all lists to the same size
for i in tokenized7.values:
    if len(i) > max_len7:
        max_len7 = len(i)
max_len7 =300
padded7 = np.array([i + [0]*(max_len7-len(i)) for i in tokenized7.values])

np.array(padded7).shape # Dimensions of the padded variable
#Masking
attention_mask7 = np.where(padded7 != 0, 1, 0)
attention_mask7.shape
input_ids7 = torch.tensor(padded7)  
attention_mask7 = torch.tensor(attention_mask7)

#Running the model () function through BERT
input_segments7= np.array([_get_segments3(token, max_len7)for token in tokenized7.values])
token_type_ids7 = torch.tensor(input_segments7)
input_segments7 = torch.tensor(input_segments7)

with torch.no_grad():
    last_hidden_states7 = model(input_ids7, attention_mask=attention_mask7, token_type_ids=input_segments7)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features7 = last_hidden_states7[0][:,0,:].numpy()
features7

"""# ***df8***"""

#Tokenization
pair8= df8['Title_1'] + [" [SEP] "] + df8['Title_2']  
tokenized8 = pair8.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len8 = 0                 # padding all lists to the same size
for i in tokenized8.values:
    if len(i) > max_len8:
        max_len8 = len(i)
max_len8 =300
padded8 = np.array([i + [0]*(max_len8-len(i)) for i in tokenized8.values])

np.array(padded8).shape # Dimensions of the padded variable
#Masking
attention_mask8 = np.where(padded8 != 0, 1, 0)
attention_mask8.shape
input_ids8 = torch.tensor(padded8)  
attention_mask8 = torch.tensor(attention_mask8)

#Running the model () function through BERT
input_segments8= np.array([_get_segments3(token, max_len8)for token in tokenized8.values])
token_type_ids8 = torch.tensor(input_segments8)
input_segments8 = torch.tensor(input_segments8)

with torch.no_grad():
    last_hidden_states8 = model(input_ids8, attention_mask=attention_mask8, token_type_ids=input_segments8)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features8 = last_hidden_states8[0][:,0,:].numpy()
features8

"""# ***df9***"""

#Tokenization
pair9= df9['Title_1'] +  [" [SEP] "] + df9['Title_2'] 
tokenized9 = pair9.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len9 = 0                 # padding all lists to the same size
for i in tokenized9.values:
    if len(i) > max_len9:
        max_len9 = len(i)
max_len9 =300
padded9 = np.array([i + [0]*(max_len9-len(i)) for i in tokenized9.values])

np.array(padded9).shape # Dimensions of the padded variable
#Masking
attention_mask9 = np.where(padded9 != 0, 1, 0)
attention_mask9.shape
input_ids9 = torch.tensor(padded9)  
attention_mask9 = torch.tensor(attention_mask9)

#Running the model () function through BERT
input_segments9= np.array([_get_segments3(token, max_len9)for token in tokenized9.values])
token_type_ids9 = torch.tensor(input_segments9)
input_segments9 = torch.tensor(input_segments9)

with torch.no_grad():
    last_hidden_states9 = model(input_ids9, attention_mask=attention_mask9, token_type_ids=input_segments9)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features9 = last_hidden_states9[0][:,0,:].numpy()
features9

"""# **df10**"""

#Tokenization
pair10= df10['Title_1'] + [" [SEP] "] + df10['Title_2'] 
tokenized10 = pair10.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len10 = 0                 # padding all lists to the same size
for i in tokenized10.values:
    if len(i) > max_len10:
        max_len10 = len(i)
max_len10 =300
padded10 = np.array([i + [0]*(max_len10-len(i)) for i in tokenized10.values])

np.array(padded10).shape # Dimensions of the padded variable
#Masking
attention_mask10 = np.where(padded10 != 0, 1, 0)
attention_mask10.shape
input_ids10 = torch.tensor(padded10)  
attention_mask10 = torch.tensor(attention_mask10)

#Running the model () function through BERT
input_segments10 = np.array([_get_segments3(token, max_len10)for token in tokenized10.values])
token_type_ids10 = torch.tensor(input_segments10)
input_segments10 = torch.tensor(input_segments10)

with torch.no_grad():
    last_hidden_states10 = model(input_ids10, attention_mask=attention_mask10, token_type_ids=input_segments10)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features10 = last_hidden_states10[0][:,0,:].numpy()
features10

"""# **df11**"""

#Tokenization
pair11= df11['Title_1'] +  [" [SEP] "] + df11['Title_2']  
tokenized11 = pair11.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len11 = 0                 # padding all lists to the same size
for i in tokenized11.values:
    if len(i) > max_len11:
        max_len11 = len(i)
max_len11 =300
padded11 = np.array([i + [0]*(max_len11-len(i)) for i in tokenized11.values])

np.array(padded11).shape # Dimensions of the padded variable
#Masking
attention_mask11 = np.where(padded11 != 0, 1, 0)
attention_mask11.shape
input_ids11 = torch.tensor(padded11)  
attention_mask11 = torch.tensor(attention_mask11)

#Running the model () function through BERT
input_segments11 = np.array([_get_segments3(token, max_len11)for token in tokenized11.values])
token_type_ids11 = torch.tensor(input_segments11)
input_segments11 = torch.tensor(input_segments11)

with torch.no_grad():
    last_hidden_states11 = model(input_ids11, attention_mask=attention_mask11, token_type_ids=input_segments11)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features11 = last_hidden_states11[0][:,0,:].numpy()
features11

"""# **df12**"""

#Tokenization
pair12= df12['Title_1'] + [" [SEP] "] + df12['Title_2'] 
tokenized12 = pair12.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len12 = 0                 # padding all lists to the same size
for i in tokenized12.values:
    if len(i) > max_len12:
        max_len12 = len(i)
max_len12 =300
padded12 = np.array([i + [0]*(max_len12-len(i)) for i in tokenized12.values])

np.array(padded12).shape # Dimensions of the padded variable
#Masking
attention_mask12 = np.where(padded12 != 0, 1, 0)
attention_mask12.shape
input_ids12 = torch.tensor(padded12)  
attention_mask12 = torch.tensor(attention_mask12)

#Running the model () function through BERT
input_segments12 = np.array([_get_segments3(token, max_len12)for token in tokenized12.values])
token_type_ids12 = torch.tensor(input_segments12)
input_segments12 = torch.tensor(input_segments12)

with torch.no_grad():
    last_hidden_states12 = model(input_ids12, attention_mask=attention_mask12, token_type_ids=input_segments12)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features12 = last_hidden_states12[0][:,0,:].numpy()
features12

"""# **df13**"""

#Tokenization
pair13= df13['Title_1'] +  [" [SEP] "] + df13['Title_2']  
tokenized13 = pair13.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len13 = 0                 # padding all lists to the same size
for i in tokenized13.values:
    if len(i) > max_len13:
        max_len13 = len(i)
max_len13 =300
padded13 = np.array([i + [0]*(max_len13-len(i)) for i in tokenized13.values])

np.array(padded13).shape # Dimensions of the padded variable
#Masking
attention_mask13 = np.where(padded13 != 0, 1, 0)
attention_mask13.shape
input_ids13 = torch.tensor(padded13)  
attention_mask13 = torch.tensor(attention_mask13)

#Running the model () function through BERT
input_segments13 = np.array([_get_segments3(token, max_len13)for token in tokenized13.values])
token_type_ids13 = torch.tensor(input_segments13)
input_segments13 = torch.tensor(input_segments13)

with torch.no_grad():
    last_hidden_states13 = model(input_ids13, attention_mask=attention_mask13, token_type_ids=input_segments13)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features13 = last_hidden_states13[0][:,0,:].numpy()
features13

"""# **df14**"""

#Tokenization
pair14= df14['Title_1'] +  [" [SEP] "] + df14['Title_2'] 
tokenized14 = pair14.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len14 = 0                 # padding all lists to the same size
for i in tokenized14.values:
    if len(i) > max_len14:
        max_len14 = len(i)
max_len14 =300
padded14 = np.array([i + [0]*(max_len14-len(i)) for i in tokenized14.values])

np.array(padded14).shape # Dimensions of the padded variable
#Masking
attention_mask14 = np.where(padded14 != 0, 1, 0)
attention_mask14.shape
input_ids14 = torch.tensor(padded14)  
attention_mask14 = torch.tensor(attention_mask14)

#Running the model () function through BERT
input_segments14 = np.array([_get_segments3(token, max_len14)for token in tokenized14.values])
token_type_ids14 = torch.tensor(input_segments14)
input_segments14 = torch.tensor(input_segments14)

with torch.no_grad():
    last_hidden_states14 = model(input_ids14, attention_mask=attention_mask14, token_type_ids=input_segments14)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features14 = last_hidden_states14[0][:,0,:].numpy()
features14

"""# **df15**"""

#Tokenization
pair15= df15['Title_1'] +  [" [SEP] "] + df15['Title_2']  
tokenized15 = pair15.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len15 = 0                 # padding all lists to the same size
for i in tokenized15.values:
    if len(i) > max_len15:
        max_len15 = len(i)
max_len15 =300
padded15 = np.array([i + [0]*(max_len15-len(i)) for i in tokenized15.values])

np.array(padded15).shape # Dimensions of the padded variable
#Masking
attention_mask15 = np.where(padded15 != 0, 1, 0)
attention_mask15.shape
input_ids15 = torch.tensor(padded15)  
attention_mask15 = torch.tensor(attention_mask15)

#Running the model () function through BERT
input_segments15 = np.array([_get_segments3(token, max_len15)for token in tokenized15.values])
token_type_ids15 = torch.tensor(input_segments15)
input_segments15 = torch.tensor(input_segments15)

with torch.no_grad():
    last_hidden_states15 = model(input_ids15, attention_mask=attention_mask15, token_type_ids=input_segments15)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features15 = last_hidden_states15[0][:,0,:].numpy()
features15

"""# **df16**"""

#Tokenization
pair16 = df16['Title_1'] +  [" [SEP] "] + df16['Title_2']  
tokenized16 = pair16.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len16 = 0                 # padding all lists to the same size
for i in tokenized16.values:
    if len(i) > max_len16:
        max_len16 = len(i)
max_len16 =300
padded16 = np.array([i + [0]*(max_len16-len(i)) for i in tokenized16.values])

np.array(padded16).shape # Dimensions of the padded variable
#Masking
attention_mask16 = np.where(padded16 != 0, 1, 0)
attention_mask16.shape
input_ids16 = torch.tensor(padded16)  
attention_mask16 = torch.tensor(attention_mask16)

#Running the model () function through BERT
input_segments16 = np.array([_get_segments3(token, max_len16)for token in tokenized16.values])
token_type_ids16 = torch.tensor(input_segments16)
input_segments16 = torch.tensor(input_segments16)

with torch.no_grad():
    last_hidden_states16 = model(input_ids16, attention_mask=attention_mask16, token_type_ids=input_segments16)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features16 = last_hidden_states16[0][:,0,:].numpy()
features16

"""# **df17**"""

#Tokenization
pair17 = df17['Title_1'] + [" [SEP] "] + df17['Title_2']  
tokenized17 = pair17.apply((lambda x: tokenizer.encode(x, add_special_tokens=True,truncation=True, max_length=300)))
#Padding
max_len17 = 0                 # padding all lists to the same size
for i in tokenized17.values:
    if len(i) > max_len17:
        max_len17 = len(i)
max_len17 =300
padded17 = np.array([i + [0]*(max_len17-len(i)) for i in tokenized17.values])

np.array(padded17).shape # Dimensions of the padded variable
#Masking
attention_mask17 = np.where(padded17 != 0, 1, 0)
attention_mask17.shape
input_ids17 = torch.tensor(padded17)  
attention_mask17 = torch.tensor(attention_mask17)

#Running the model () function through BERT
input_segments17 = np.array([_get_segments3(token, max_len17)for token in tokenized17.values])
token_type_ids17 = torch.tensor(input_segments17)
input_segments17 = torch.tensor(input_segments17)

with torch.no_grad():
    last_hidden_states17 = model(input_ids17, attention_mask=attention_mask17, token_type_ids=input_segments17)    # <<< 600 rows only !!!
#Slicing the part of the output of BERT : [cls]
features17 = last_hidden_states17[0][:,0,:].numpy()
features17

"""# ***Classification***"""

features=np.concatenate([features1,features2,features3,features4,features5,features6,features7,features8,features9,features10,features11,features12,features13,features14,features15,features16,features17])

features.shape

Total = pd.concat([df1,df2,df3,df4,df5,df6,df7,df8,df9,df10,df11,df12,df13,df14,df15,df16,df17], ignore_index=True)

labels =Total['Label']
labels

"""# ***Hold out***"""

train_features = features[:2662]
train_labels = labels[:2662]
test_features = features[2662:]
test_labels = labels[2662:]

train_features

train_labels

test_features

test_labels

#train_features,test_features,train_labels,test_labels= train_test_split(features,labels,test_size=0.2, random_state=2)
#print('Number of data points in train data:', train_features.shape[0])
#print('Number of data points in train data:', train_labels.shape[0])
#print('Number of data points in test data:', train_features.shape[0])
#print('Number of data points in test data:', train_labels.shape[0])
#print(features.shape,train_features.shape,test_features.shape)

import matplotlib.pyplot as plt
import seaborn as sns

def plot_confusion_matrix(test_labels, y_predLr):
    C = confusion_matrix(test_labels, y_predLr)
    A =(((C.T)/(C.sum(axis=1))).T)
    B =(C/C.sum(axis=0))
    plt.figure(figsize=(20,4))
    labels = [0,1]
    plt.subplot(1, 3, 1)
    sns.heatmap(C, annot=True, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Confusion matrix")
    
    plt.subplot(1, 3, 2)
    sns.heatmap(B, annot=True,  fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Precision matrix")
    
    plt.subplot(1, 3, 3)
    # representing B in heatmap format
    sns.heatmap(A, annot=True, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Recall matrix")
    
    plt.show()

"""# ***LogisticRegression Optimized***"""

#n_splits=2
#cross_val_score=5
parameters = {'C': np.linspace(0.0001, 100, 20)}
grid_search = GridSearchCV(LogisticRegression(), parameters, cv=5)
grid_search.fit(train_features, train_labels)
print('best parameters: ', grid_search.best_params_)
print('best scrores: ', grid_search.best_score_)

lr_clf = LogisticRegression(C=10.52)
lr_clf.fit(train_features, train_labels)

lr_clf.score(test_features, test_labels)

y_predLr = lr_clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_predLr

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_predLr))

print(confusion_matrix(test_labels, y_predLr))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_predLr))

plot_confusion_matrix(test_labels, y_predLr)

scores = cross_val_score(lr_clf, features, labels,cv=5)
print("mean: {:.3f} (std: {:.3f})".format(scores.mean(),
                                          scores.std()),
                                          end="\n\n" )

"""# ***Decision tree***"""

from sklearn.tree import DecisionTreeClassifier
clf = DecisionTreeClassifier(max_depth = 500, random_state = 0)
clf.fit(train_features, train_labels)

y_preddt = clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_preddt

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_preddt))

print(confusion_matrix(test_labels, y_preddt))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_preddt))

# The score method returns the accuracy of the model
score = clf.score(test_features, test_labels)
print(score)

plot_confusion_matrix(test_labels, y_preddt)

scores = cross_val_score(clf, features, labels, cv=5)
print("mean: {:.3f} (std: {:.3f})".format(scores.mean(),
                                          scores.std()),
                                          end="\n\n" )

"""# ***SVM***"""

from sklearn.svm import SVC
svclassifier = SVC(kernel='linear')
svclassifier.fit(train_features, train_labels)

y_pred = svclassifier.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_pred))

print(confusion_matrix(test_labels,y_pred))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_pred))

plot_confusion_matrix(test_labels,y_pred)

scores = cross_val_score(svclassifier,features, labels, cv=5)
print("mean: {:.3f} (std: {:.3f})".format(scores.mean(),
                                          scores.std()),
                                          end="\n\n" )

"""# ***MLP Best params***"""

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(max_iter=300)
from sklearn.datasets import make_classification

parameter_space = {
    'hidden_layer_sizes': [(150,100,50), (120,80,40), (100,50,30)],
    'max_iter': [50, 100, 150],
    'activation': ['tanh', 'relu'],
    'solver': ['sgd', 'adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
    }

from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=5)
clf.fit(train_features, train_labels)

# Best paramete set
print('Best parameters found:\n', clf.best_params_)

# All results
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

y_true, y_pred = test_labels , clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report, confusion_matrix
print('Results on the test set:')
print(classification_report(y_true, y_pred))

print(confusion_matrix(y_true, y_pred))

y_true, y_pred = test_labels , clf.predict(test_features)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_true, y_pred))
clf.score(test_features, test_labels)

plot_confusion_matrix(y_true, y_pred)

"""# ***Random Forest***"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=20, random_state=0)
rf.fit(train_features, train_labels)
y_pred1 = rf.predict(test_features)
y_pred1

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_pred1))

print(confusion_matrix(test_labels, y_pred1))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_pred1))

plot_confusion_matrix(test_labels, y_pred1)

"""# ***Naive Bayes***"""

from sklearn.naive_bayes import GaussianNB
gnb = GaussianNB()
gnb.fit(train_features, train_labels)

y_pred = gnb.predict(test_features)
y_pred

from sklearn import metrics
print("Accuracy:",metrics.accuracy_score(test_labels, y_pred))

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,y_pred))

print(confusion_matrix(test_labels, y_pred))

plot_confusion_matrix(test_labels, y_pred)

"""# ***XGBoost***"""

import xgboost as xgb
modelxgb=xgb.XGBClassifier(random_state=1,learning_rate=0.01)
modelxgb.fit(train_features, train_labels)

predxgb = modelxgb.predict(test_features)
predxgb

from sklearn.metrics import classification_report, confusion_matrix
print(classification_report(test_labels,predxgb))

print(confusion_matrix(test_labels, predxgb))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, predxgb))
modelxgb.score(test_features,test_labels)

plot_confusion_matrix(test_labels, predxgb)

"""# ***KNN***"""

#import KNeighborsClassifier
from sklearn.neighbors import KNeighborsClassifier
#Setup arrays to store training and test accuracies
neighbors = np.arange(1,9)
train_accuracy =np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))

for i,k in enumerate(neighbors):
    #Setup a knn classifier with k neighbors
    knn = KNeighborsClassifier(n_neighbors=k)
    
    #Fit the model
    knn.fit(train_features, train_labels)
    
    #Compute accuracy on the training set
    train_accuracy[i] = knn.score(train_features, train_labels)
    
    #Compute accuracy on the test set
    test_accuracy[i] = knn.score(test_features, test_labels)

import matplotlib.pyplot as plt
plt.style.use('ggplot')
#Generate plot
plt.title('k-NN Varying number of neighbors')
plt.plot(neighbors, test_accuracy, label='Testing Accuracy')
plt.plot(neighbors, train_accuracy, label='Training accuracy')
plt.legend()
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
plt.show()

knn = KNeighborsClassifier(n_neighbors=7)
#Fit the model
knn.fit(train_features,train_labels)

knn.score(test_features,test_labels)

y_pred = knn.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report
print(classification_report(test_labels,y_pred))

from sklearn.metrics import confusion_matrix
print(confusion_matrix(test_labels,y_pred))

from sklearn.metrics import accuracy_score
print(accuracy_score(test_labels, y_pred))

plot_confusion_matrix(test_labels,y_pred)

"""# ***mlp***"""

from sklearn import datasets
from sklearn import metrics
from sklearn.neural_network import MLPClassifier
from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use('ggplot')

model = MLPClassifier()
model.fit(train_features, train_labels)
print(model)

expected_y  = test_labels
predicted_y = model.predict(test_features)

print(metrics.classification_report(expected_y, predicted_y))

print(metrics.confusion_matrix(expected_y, predicted_y))

expected_y, predicted_y = test_labels , clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
predicted_y

plot_confusion_matrix(expected_y, predicted_y)



"""https://www.projectpro.io/recipes/use-mlp-classifier-and-regressor-in-python

https://python-course.eu/machine-learning/neural-networks-with-scikit.php
"""

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


from sklearn.neural_network import MLPClassifier

from sklearn.metrics import accuracy_score
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report

from sklearn.model_selection import GridSearchCV

sc=StandardScaler()

scaler = sc.fit(train_features)
trainX_scaled = scaler.transform(train_features)
testX_scaled = scaler.transform(test_features)

mlp_clf = MLPClassifier(hidden_layer_sizes=(5,2),
                        max_iter = 300,activation = 'relu',
                        solver = 'adam')

mlp_clf = MLPClassifier(hidden_layer_sizes=(150,100,50),
                        max_iter = 300,activation = 'relu',
                        solver = 'adam')

mlp_clf.fit(trainX_scaled, train_labels)

y_pred = mlp_clf.predict(testX_scaled)

print('Accuracy: {:.2f}'.format(accuracy_score(test_labels, y_pred)))

print(classification_report(test_labels, y_pred))

y_pred = modelxgb.predict(test_features)
y_pred

fig = plot_confusion_matrix(mlp_clf, testX_scaled, test_labels, display_labels=mlp_clf.classes_)
fig.figure_.suptitle("Confusion Matrix for Winequality Dataset")
plt.show()

"""https://michael-fuchs-python.netlify.app/2021/02/03/nn-multi-layer-perceptron-classifier-mlpclassifier/#mlpclassifier-for-binary-classification

## ***MLP Best params1***
"""

from sklearn.neural_network import MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(5,2), max_iter = 300,activation = 'relu', solver = 'adam')
from sklearn.datasets import make_classification

parameter_space = {
    'hidden_layer_sizes': [(150,100,50)],
    'activation': ['relu'],
    'solver': ['adam'],
    'alpha': [0.0001, 0.05],
    'learning_rate': ['constant','adaptive'],
}

from sklearn.model_selection import GridSearchCV
clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)
clf.fit(train_features, train_labels)

# Best paramete set
print('Best parameters found:\n', clf.best_params_)

# All results
means = clf.cv_results_['mean_test_score']
stds = clf.cv_results_['std_test_score']
for mean, std, params in zip(means, stds, clf.cv_results_['params']):
    print("%0.3f (+/-%0.03f) for %r" % (mean, std * 2, params))

y_true, y_pred = test_labels , clf.predict(test_features)
np.set_printoptions(threshold=np.inf)
y_pred

from sklearn.metrics import classification_report, confusion_matrix
print('Results on the test set:')
print(classification_report(y_true, y_pred))

print(confusion_matrix(y_true, y_pred))

y_true, y_pred = test_labels , clf.predict(test_features)

from sklearn.metrics import accuracy_score
print(accuracy_score(y_true, y_pred))
clf.score(test_features, test_labels)

def plot_confusion_matrix(test_labels, y_predLr):
    C = confusion_matrix(test_labels, y_predLr)
    A =(((C.T)/(C.sum(axis=1))).T)
    B =(C/C.sum(axis=0))
    plt.figure(figsize=(20,4))
    labels = [0,1]
    plt.subplot(1, 3, 1)
    sns.heatmap(C, annot=True, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Confusion matrix")
    
    plt.subplot(1, 3, 2)
    sns.heatmap(B, annot=True,  fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Precision matrix")
    
    plt.subplot(1, 3, 3)
    # representing B in heatmap format
    sns.heatmap(A, annot=True, fmt=".3f", xticklabels=labels, yticklabels=labels)
    plt.xlabel('Predicted Class')
    plt.ylabel('Original Class')
    plt.title("Recall matrix")
    
    plt.show()

plot_confusion_matrix(y_true, y_pred)